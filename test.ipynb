{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "524f2cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subbasin 1.0, Parameter resolution: 47 values interpolated\n",
      "Subbasin 1.0, Parameter dataset: 47 values interpolated\n",
      "Subbasin 1.0, Parameter parameter: 47 values interpolated\n",
      "Subbasin 1.0, Parameter value: 47 values interpolated\n",
      "Subbasin 1.0, Parameter quality: 47 values interpolated\n",
      "Stored interpolated parameters for subbasin '1.0' in all_results\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from src.dwd import dwd_daily_met_distance_plus_solar_rank\n",
    "from src.idw import idw_from_distances\n",
    "\n",
    "# Set the paths manually\n",
    "BASE_DIR = Path(\"/home/ghnfarid/Projects/meteo-interp\")\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "# store results for all subbasins: { subbasin: { param: DataFrame } }\n",
    "all_results = {}\n",
    "\n",
    "# Now read the files\n",
    "watershed = pd.read_excel(DATA_DIR / \"watershed.xlsx\")\n",
    "interpolation_pars = pd.read_excel(DATA_DIR / \"interpolation_parameters.xlsx\")\n",
    "richter_pars = pd.read_excel(DATA_DIR / \"richter_parameters.xlsx\")\n",
    "\n",
    "\n",
    "for index, row in watershed.iterrows():\n",
    "    subbasin = row[\"Subbasin\"]\n",
    "    latlon = (row[\"Lat\"], row[\"Long\"])\n",
    "\n",
    "    # ---- DWD Data Fetching ----\n",
    "    start_date = interpolation_pars['start_date'].iloc[0].strftime(\"%Y-%m-%d\")\n",
    "    end_date = interpolation_pars['end_date'].iloc[0].strftime(\"%Y-%m-%d\")\n",
    "    distance_km = interpolation_pars['radius_kl'].iloc[0]\n",
    "    periods = [\"historical\"]\n",
    "\n",
    "    # Fetch stations and values from DWD\n",
    "    stations_df, values_df = dwd_daily_met_distance_plus_solar_rank(\n",
    "        latlon=latlon,\n",
    "        distance_km=distance_km,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        periods=periods,\n",
    "        drop_nulls=True,\n",
    "    )\n",
    "\n",
    "    if len(stations_df) == 0:\n",
    "        print(f\"No stations found for subbasin {subbasin}\")\n",
    "        continue\n",
    "\n",
    "    # Convert to pandas for easier manipulation\n",
    "    stations_pd = stations_df.to_pandas() if hasattr(stations_df, 'to_pandas') else stations_df\n",
    "    values_pd = values_df.to_pandas() if hasattr(values_df, 'to_pandas') else values_df\n",
    "\n",
    "    # Get all parameter columns (exclude date/time and station metadata)\n",
    "    exclude_cols = {'station_id', 'latitude', 'longitude', 'distance', 'date', 'start_date', 'end_date'}\n",
    "    parameter_cols = [col for col in values_pd.columns if col not in exclude_cols]\n",
    "\n",
    "    # For each parameter, perform IDW interpolation\n",
    "    parameter_dfs = {}\n",
    "\n",
    "    for param in parameter_cols:\n",
    "        interpolated_values = []\n",
    "        dates = []\n",
    "\n",
    "        for date in values_pd['date'].unique():\n",
    "            date_data = values_pd[values_pd['date'] == date]\n",
    "\n",
    "            # Get distances and values for stations on this date\n",
    "            distances = []\n",
    "            param_values = []\n",
    "\n",
    "            for station_id in date_data['station_id'].values:\n",
    "                station_distance = stations_pd[stations_pd['station_id'] == station_id]['distance'].values\n",
    "                if len(station_distance) > 0:\n",
    "                    distances.append(station_distance[0])\n",
    "                    param_val = date_data[date_data['station_id'] == station_id][param].values\n",
    "                    if len(param_val) > 0:\n",
    "                        param_values.append(param_val[0])\n",
    "\n",
    "            # Perform IDW interpolation if we have valid data\n",
    "            if len(distances) > 0 and len(param_values) > 0:\n",
    "                try:\n",
    "                    interp_value = idw_from_distances(distances, param_values, power=2.0)\n",
    "                    interpolated_values.append(interp_value)\n",
    "                except (ValueError, ZeroDivisionError):\n",
    "                    # Set to -99 if interpolation fails\n",
    "                    interpolated_values.append(-99)\n",
    "            else:\n",
    "                # Set to -99 if no valid data\n",
    "                interpolated_values.append(-99)\n",
    "\n",
    "            dates.append(date)\n",
    "\n",
    "        # Create dataframe for this parameter\n",
    "        if len(dates) > 0:\n",
    "            parameter_dfs[param] = pd.DataFrame({\n",
    "                'date': dates,\n",
    "                f'{param}_interpolated': interpolated_values\n",
    "            })\n",
    "            print(f\"Subbasin {subbasin}, Parameter {param}: {len(parameter_dfs[param])} values interpolated\")\n",
    "\n",
    "    # Store parameter DataFrames separately for this subbasin (kept in memory)\n",
    "    all_results[subbasin] = parameter_dfs\n",
    "    print(f\"Stored interpolated parameters for subbasin '{subbasin}' in all_results\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4878584e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subbasin 1.0, Parameter humidity: 47 values interpolated\n",
      "Subbasin 1.0, Parameter precipitation_height: 47 values interpolated\n",
      "Subbasin 1.0, Parameter temperature_air_max_2m: 47 values interpolated\n",
      "Subbasin 1.0, Parameter temperature_air_mean_2m: 47 values interpolated\n",
      "Subbasin 1.0, Parameter temperature_air_min_2m: 47 values interpolated\n",
      "Subbasin 1.0, Parameter wind_speed: 47 values interpolated\n",
      "Subbasin 1.0, Parameter radiation_global: 47 values interpolated\n",
      "Stored interpolated parameters for subbasin '1.0' in all_results\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.dwd import dwd_daily_met_distance_plus_solar_rank\n",
    "from src.idw import idw_from_distances\n",
    "\n",
    "# Set the paths manually\n",
    "BASE_DIR = Path(\"/home/ghnfarid/Projects/meteo-interp\")\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "# store results for all subbasins: { subbasin: { param: DataFrame } }\n",
    "all_results = {}\n",
    "\n",
    "# Now read the files\n",
    "watershed = pd.read_excel(DATA_DIR / \"watershed.xlsx\")\n",
    "interpolation_pars = pd.read_excel(DATA_DIR / \"interpolation_parameters.xlsx\")\n",
    "richter_pars = pd.read_excel(DATA_DIR / \"richter_parameters.xlsx\")\n",
    "\n",
    "\n",
    "for index, row in watershed.iterrows():\n",
    "    subbasin = row[\"Subbasin\"]\n",
    "    latlon = (row[\"Lat\"], row[\"Long\"])\n",
    "\n",
    "    # ---- DWD Data Fetching ----\n",
    "    start_date = interpolation_pars['start_date'].iloc[0].strftime(\"%Y-%m-%d\")\n",
    "    end_date = interpolation_pars['end_date'].iloc[0].strftime(\"%Y-%m-%d\")\n",
    "    distance_km = interpolation_pars['radius_kl'].iloc[0]\n",
    "    periods = [\"historical\"]\n",
    "\n",
    "    # Fetch stations and values from DWD\n",
    "    stations_df, values_df = dwd_daily_met_distance_plus_solar_rank(\n",
    "        latlon=latlon,\n",
    "        distance_km=distance_km,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        periods=periods,\n",
    "        drop_nulls=True,\n",
    "    )\n",
    "\n",
    "    if len(stations_df) == 0:\n",
    "        print(f\"No stations found for subbasin {subbasin}\")\n",
    "        continue\n",
    "\n",
    "    # Convert to pandas for easier manipulation\n",
    "    stations_pd = stations_df.to_pandas() if hasattr(stations_df, 'to_pandas') else stations_df\n",
    "    values_pd = values_df.to_pandas() if hasattr(values_df, 'to_pandas') else values_df\n",
    "    # Support both wide and long (tidy) formats from wetterdienst\n",
    "    def _infer_value_columns(df: pd.DataFrame):\n",
    "        cols = {c.lower(): c for c in df.columns}\n",
    "        station_col = cols.get('station_id') or cols.get('stations_id') or cols.get('stationid')\n",
    "        date_col = cols.get('date') or cols.get('mess_datum') or cols.get('datetime') or cols.get('time')\n",
    "        param_col = cols.get('parameter') or cols.get('element') or cols.get('observation_type')\n",
    "        value_col = cols.get('value') or cols.get('wert') or cols.get('measurement')\n",
    "        return station_col, date_col, param_col, value_col\n",
    "\n",
    "    # Detect long vs wide format\n",
    "    lower_cols = set(c.lower() for c in values_pd.columns)\n",
    "    is_long = any(x in lower_cols for x in ('parameter', 'element', 'observation_type'))\n",
    "\n",
    "    parameter_dfs = {}\n",
    "\n",
    "    if is_long:\n",
    "        station_col, date_col, param_col, value_col = _infer_value_columns(values_pd)\n",
    "        if not all([station_col, date_col, param_col, value_col]):\n",
    "            raise KeyError(f\"Could not infer columns from values_df: {list(values_pd.columns)}\")\n",
    "\n",
    "        # ensure datetime\n",
    "        values_pd[date_col] = pd.to_datetime(values_pd[date_col])\n",
    "\n",
    "        params = values_pd[param_col].unique()\n",
    "        for param in params:\n",
    "            df_rows = []\n",
    "            dates = sorted(values_pd[date_col].unique())\n",
    "            for d in dates:\n",
    "                subset = values_pd[(values_pd[date_col] == d) & (values_pd[param_col] == param)]\n",
    "                # map station -> distance\n",
    "                distances = []\n",
    "                vals = []\n",
    "                for _, r in subset.iterrows():\n",
    "                    sid = r[station_col]\n",
    "                    station_distance = stations_pd[stations_pd['station_id'] == sid]['distance'].values\n",
    "                    if len(station_distance) == 0:\n",
    "                        continue\n",
    "                    v = r[value_col]\n",
    "                    if pd.isna(v):\n",
    "                        continue\n",
    "                    distances.append(float(station_distance[0]))\n",
    "                    vals.append(float(v))\n",
    "\n",
    "                if len(distances) > 0 and len(vals) > 0:\n",
    "                    try:\n",
    "                        val = idw_from_distances(distances, vals, power=2.0)\n",
    "                    except Exception:\n",
    "                        val = -99\n",
    "                else:\n",
    "                    val = -99\n",
    "                df_rows.append({'date': d, 'value': val})\n",
    "\n",
    "            parameter_dfs[param] = pd.DataFrame(df_rows)\n",
    "            print(f\"Subbasin {subbasin}, Parameter {param}: {len(parameter_dfs[param])} values interpolated\")\n",
    "    else:\n",
    "        # wide format: parameter columns are all except known metadata\n",
    "        exclude_cols = {'station_id', 'latitude', 'longitude', 'distance', 'date', 'start_date', 'end_date'}\n",
    "        parameter_cols = [col for col in values_pd.columns if col not in exclude_cols]\n",
    "\n",
    "        for param in parameter_cols:\n",
    "            df_rows = []\n",
    "            dates = sorted(values_pd['date'].unique())\n",
    "            for d in dates:\n",
    "                date_data = values_pd[values_pd['date'] == d]\n",
    "                distances = []\n",
    "                vals = []\n",
    "                for _, r in date_data.iterrows():\n",
    "                    sid = r.get('station_id')\n",
    "                    station_distance = stations_pd[stations_pd['station_id'] == sid]['distance'].values\n",
    "                    if len(station_distance) == 0:\n",
    "                        continue\n",
    "                    v = r.get(param)\n",
    "                    if pd.isna(v):\n",
    "                        continue\n",
    "                    distances.append(float(station_distance[0]))\n",
    "                    vals.append(float(v))\n",
    "\n",
    "                if len(distances) > 0 and len(vals) > 0:\n",
    "                    try:\n",
    "                        val = idw_from_distances(distances, vals, power=2.0)\n",
    "                    except Exception:\n",
    "                        val = -99\n",
    "                else:\n",
    "                    val = -99\n",
    "                df_rows.append({'date': d, 'value': val})\n",
    "\n",
    "            parameter_dfs[param] = pd.DataFrame(df_rows)\n",
    "            print(f\"Subbasin {subbasin}, Parameter {param}: {len(parameter_dfs[param])} values interpolated\")\n",
    "\n",
    "    # Store parameter DataFrames separately for this subbasin (kept in memory)\n",
    "    all_results[subbasin] = parameter_dfs\n",
    "    print(f\"Stored interpolated parameters for subbasin '{subbasin}' in all_results\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042ab9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "humidity_df = parameter_dfs['humidity']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meteo-interp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
